{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical"
      ],
      "metadata": {
        "id": "RANw65zS-hHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is a Decision Tree, and how does it work?"
      ],
      "metadata": {
        "id": "TJcq6CUe-iJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. A Decision Tree is a supervised learning algorithm used for classification and regression. It works by recursively splitting data based on the feature that best separates it, forming a tree-like structure. Each internal node represents a decision, branches represent possible outcomes, and leaf nodes contain final predictions."
      ],
      "metadata": {
        "id": "EUFeKEEX-m69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are impurity measures in Decision Trees?"
      ],
      "metadata": {
        "id": "25V_qNYW-17c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Impurity measures in Decision Trees determine how mixed a node is. Lower impurity means better splits.\n",
        "\n",
        "Common Impurity Measures :\n",
        "\n",
        "1. Gini Impurity (used in CART)\n",
        "‚Ä¢ Measures how often a randomly chosen element would be misclassified.\n",
        "‚Ä¢ Formula: ùê∫ ùëñ ùëõ ùëñ = 1 ‚àí ‚àë ùëù ùëñ 2 Gini=1‚àí‚àëp i 2‚Äã\n",
        "\n",
        "2. Entropy (used in ID3, C4.5)\n",
        "\n",
        "‚Ä¢ Measures the uncertainty in data.\n",
        "‚Ä¢ Formula: ùê∏ ùëõ ùë° ùëü ùëú ùëù ùë¶ = ‚àí ‚àë ùëù ùëñ log ‚Å° 2 ùëù ùëñ Entropy=‚àí‚àëp i‚Äãlog 2‚Äãp i‚Äã\n",
        "\n",
        "3. Variance Reduction (for regression)\n",
        "\n",
        "‚Ä¢ Measures how much variance decreases after a split."
      ],
      "metadata": {
        "id": "bnD2U94d_UQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is the mathematical formula for Gini Impurity?"
      ],
      "metadata": {
        "id": "_nf2ohp2_BKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The Gini Impurity formula is:\n",
        "\n",
        "Gini=1‚àí‚àëp i 2‚Äã\n",
        "\n",
        "Where:\n",
        "‚Ä¢ p i‚Äã= Probability of class ùëñ i in the node.\n",
        "‚Ä¢ The sum runs over all classes in the node."
      ],
      "metadata": {
        "id": "GL99tPNwAV8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the mathematical formula for Entropy?"
      ],
      "metadata": {
        "id": "bP47pN3wApDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The Entropy formula is:\n",
        "\n",
        "Entropy=‚àí‚àëp i‚Äãlog 2‚Äãp i‚Äã\n",
        "\n",
        "Where:\n",
        "‚Ä¢ p i‚Äã= Probability of class ùëñ i in the node.\n",
        "‚Ä¢ The sum runs over all classes in the node."
      ],
      "metadata": {
        "id": "dhw85xtNAtYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "F5XiD06dBD93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Information Gain (IG) measures how much uncertainty (entropy) is reduced after a split in a Decision Tree. It helps choose the best feature to split on.\n",
        "\n",
        "Formula:\n",
        "\n",
        "IG=Entropy(parent)‚àí‚àë( N parent‚Äã\n",
        "\n",
        "N child‚Äã\n",
        "\n",
        "‚Äã√óEntropy(child))\n",
        "\n",
        "Where:\n",
        "‚Ä¢ Higher IG means a better split.\n",
        "‚Ä¢ The feature with the highest IG is selected for splitting."
      ],
      "metadata": {
        "id": "OyJSZD22BH9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "9QW5xTolBfNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Gini Impurity vs. Entropy:\n",
        "\n",
        "‚Ä¢ Gini Impurity measures misclassification probability (faster to compute).\n",
        "‚Ä¢ Entropy measures data uncertainty (slower due to log calculations).\n",
        "‚Ä¢ Gini is used in CART, while Entropy is used in ID3, C4.5.\n",
        "‚Ä¢ Both give similar results, but Gini is more efficient."
      ],
      "metadata": {
        "id": "JpEWDxHvBkBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What is the mathematical explanation behind Decision Trees?"
      ],
      "metadata": {
        "id": "B8ga61qYCDrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Mathematical Explanation of Decision Trees\n",
        "\n",
        "1. Splitting: Choose the best feature using Information Gain (IG) or Gini Impurity.\n",
        "2. Recursive Partitioning: Keep splitting until stopping conditions are met.\n",
        "3. Prediction:\n",
        "\n",
        "‚Ä¢ Classification: Majority class in the leaf node.\n",
        "‚Ä¢ Regression: Mean of values in the leaf node.\n",
        "\n",
        "4. Pruning: Reduces overfitting by trimming weak branches.\n",
        "\n",
        "Key Idea: Decision Trees use entropy reduction or impurity minimization to make hierarchical decisions!"
      ],
      "metadata": {
        "id": "MqWT9ClECHv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "-XqyiMrdClZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Pre-Pruning stops Decision Tree growth early to prevent overfitting.\n",
        "\n",
        "Stops Splitting If:\n",
        "\n",
        "‚Ä¢ Max depth is reached.\n",
        "‚Ä¢ Min samples per leaf are too few.\n",
        "‚Ä¢  gain is too low."
      ],
      "metadata": {
        "id": "1KvlXxOVCqVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is Post-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "44MTivbiDDn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Post-Pruning trims a fully grown Decision Tree after training to prevent overfitting.\n",
        "\n",
        "How it Works:\n",
        "1. Grow the full tree.\n",
        "2. Use validation data to evaluate branches.\n",
        "3. Remove weak nodes that don‚Äôt improve performance."
      ],
      "metadata": {
        "id": "IOxH42XoDH7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the difference between Pre-Pruning and Post-Pruning?"
      ],
      "metadata": {
        "id": "ZquthfyZDdWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Pre-Pruning vs. Post-Pruning\n",
        "\n",
        "‚Ä¢ Pre-Pruning stops tree growth early (during training).\n",
        "‚Ä¢ Post-Pruning removes weak branches after training.\n",
        "‚Ä¢Pre-Pruning is faster but may underfit.\n",
        "‚Ä¢ Post-Pruning is slower but reduces overfitting better.\n",
        "\n",
        "Pre = Preventive, Post = Corrective!"
      ],
      "metadata": {
        "id": "ZuFullK7DhLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What is a Decision Tree Regressor?"
      ],
      "metadata": {
        "id": "epe6k8XcD2Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Decision Tree Regressor\n",
        "\n",
        "A Decision Tree Regressor predicts continuous values by splitting data based on features to minimize error.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "‚Ä¢ Uses MSE or MAE to find the best split.\n",
        "‚Ä¢ Predicts by taking the average of values in each leaf node.\n",
        "‚Ä¢ Handles non-linearity but can overfit without pruning."
      ],
      "metadata": {
        "id": "7zQJvPk5D6bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. What are the advantages and disadvantages of Decision Trees?"
      ],
      "metadata": {
        "id": "ol6S2GyTEP6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.  Advantages:\n",
        "\n",
        "‚Ä¢ Easy to understand & interpret.\n",
        "‚Ä¢Handles both categorical & numerical data.\n",
        "‚Ä¢ No need for feature scaling.\n",
        "‚Ä¢ Captures non-linear relationships.\n",
        "‚Ä¢Automatic feature selection.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "‚Ä¢ Overfitting (needs pruning).\n",
        "‚Ä¢ Unstable (small changes affect the tree).\n",
        "‚Ä¢ Biased towards features with many levels.\n",
        "‚Ä¢ Step-like predictions in regression."
      ],
      "metadata": {
        "id": "XHdAIBC_EUoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. How does a Decision Tree handle missing values?"
      ],
      "metadata": {
        "id": "kppJTakFE6vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Handling Missing Values in Decision Trees:\n",
        "\n",
        "1. Ignore missing values in splits.\n",
        "2. Impute with mean/median (numerical) or most frequent category (categorical).\n",
        "3. Surrogate Splitting ‚Äì Use alternative features.\n",
        "4. Weighting & Probabilities ‚Äì Distribute missing values based on data patterns.\n",
        "\n",
        "Decision Trees handle missing data well!"
      ],
      "metadata": {
        "id": "HTHgf4osE-Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. How does a Decision Tree handle categorical features?"
      ],
      "metadata": {
        "id": "4JMncTcnFTb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Handling Categorical Features in Decision Trees:\n",
        "\n",
        "1. Label Encoding ‚Äì Assigns numbers to categories.\n",
        "2. One-Hot Encoding ‚Äì Converts categories into binary columns.\n",
        "3. Direct Splitting ‚Äì Splits based on category values.\n",
        "4. Gini/Entropy Calculation ‚Äì Evaluates best splits.\n",
        "\n",
        "Decision Trees handle categorical data efficiently!"
      ],
      "metadata": {
        "id": "PT4Y6k2tFW8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. What are some real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "XJlYyaj_FsB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Real-World Applications of Decision Trees:\n",
        "\n",
        "‚Ä¢ Medical Diagnosis ‚Äì Disease prediction.\n",
        "‚Ä¢ Credit Scoring ‚Äì Loan approval decisions.\n",
        "‚Ä¢ Customer Segmentation ‚Äì Targeted marketing.\n",
        "‚Ä¢ Fraud Detection ‚Äì Identifying suspicious transactions.\n",
        "‚Ä¢ Stock Market Prediction ‚Äì Trend analysis.\n",
        "‚Ä¢ Churn Prediction ‚Äì Detecting potential customer loss.\n",
        "‚Ä¢ Quality Control ‚Äì Finding defective products.\n",
        "‚Ä¢ Spam Filtering ‚Äì Classifying emails."
      ],
      "metadata": {
        "id": "sKlDvFkOFvlE"
      }
    }
  ]
}